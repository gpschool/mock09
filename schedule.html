---
title: Schedule
layout: default
---
<h2>Workshop Schedule</h2>

<table width="100%">
  <tr>
    <td width="10%" ><a name="intro"></a>07:30 - 07:45</td>
    <td width="90%"><b>Introduction</b>
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.cs.manchester.ac.uk/~neill/">Neil D. Lawrence</a>, <i>University of Manchester</i></td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk1"></a>07:45 - 08:25</td>
    <td width="90%"><b>Geostatistics	for Gaussian Processes</b>
[<a href="http://videolectures.net/nipsworkshops09_wackernagel_ggp/">video</a>]
[<a href="./slides/wackernagel.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://hans.wackernagel.free.fr/">Hans     Wackernagel</a>, <i>MINES-ParisTech</i></td>
  </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    Gaussian process methodology has inspired a number of stimulating new
ideas in the area of machine learning.
Kriging has been introduced as a statistical interpolation method for
the design of computer experiments twenty years ago. However, some
aspects of the geostatistical methodology originally developed for
natural resource estimation have been ignored when switching to this
new context. This talk reviews concepts of geostatistics and in
particular the estimation of components of spatial variation in the
context of multiple correlated outputs.

</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk2"></a>08:25 - 09:05</td>
    <td width="90%"><b>Borrowing strength, learning vector valued functions, and supervised dimension reduction</b>
[<a href="http://videolectures.net/nipsworkshops09_mukherjee_bslvvfsdr/">video</a>]
[<a href="./slides/mukherjee.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.genome.duke.edu/labs/mukherjee/">Sayan Mukherjee</a>, <i>Duke University</i></td>
  </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    We study the problem of supervised dimension reduction from the
perspective of learning vector valued functions and multi-task or
hierarchical modeling in a regularization framework. An algorithm is specified and empirical
results are provided. In the second part of the talk the same problem
of supervised dimension reduction for a
hierarchical model is revisted from a non-parametric Bayesian
perspective.
</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="cofee1"></a>09:05 - 09:30</td>
    <td width="90%"><b>Coffee Break</b>
 </td>
 </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk3"></a>09:30 - 10:10</td>
    <td width="90%"><b>Gaussian processes and process convolutions from a Bayesian Perspective</b>
[<a href="http://videolectures.net/nipsworkshops09_higdon_gppcbp/">video</a>]
[<a href="./slides/higdon.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.stat.lanl.gov/ccs6/staff/DHigdon/index.html">Dave Higdon</a>, <i>Los Alamos National Laboratory</i></td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="discussion"></a>10:10 - 10:30</td>
    <td width="90%"><b>Discussion session</b>
 </td>
 </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk1"></a>03:30 - 04:10</td>
    <td width="90%"><b>Prior	Knowledge	and  Sparse Methods for	Convolved Multiple Outputs Gaussian Processes</b>
[<a href="http://videolectures.net/nipsworkshops09_alvarez_pksmcmogp/">video</a>]
[<a href="./slides/alvarez.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://www.cs.manchester.ac.uk/~alvarezm/">Mauricio A. Alvarez</a>, <i>University of Manchester</i></td>
  </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    One approach to account for non-trivial correlations between outputs
employs convolution processes. Under a latent function interpretation
of the convolution transform it is possible to establish dependencies
between output variables. Two important aspects in this framework are
how can we introduce prior knowledge and how can we perform efficient 
inference. Relating the convolution operation with dynamical systems,
we can specify richer covariance functions for multiple outputs.
We also  present different sparse approximations for dependent 
output Gaussian processes in the context of structured covariances. 

Joint work with Neil Lawrence, David Luengo and Michalis Titsias. 
</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk2"></a>04:10 - 04:50</td>
    <td width="90%"><b>Multi-Task Learning and Matrix Regularization</b>
[<a href="http://videolectures.net/nipsworkshops09_argyriou_mtlmr/">video</a>]
[<a href="./slides/argyriou.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://ttic.uchicago.edu/~argyriou/">Andreas Argyriou</a>, <i>Toyota Technological Institute</i></td>
  </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    Multi-task learning extends the standard paradigm of
supervised learning. In multi-task learning, samples for multiple
related tasks are given and the goal is to learn a function for each
task and also to generalize well (transfer learned knowledge) on new
tasks. The applications of this paradigm are numerous and range from
computer vision to collaborative filtering to bioinformatics while it
also relates to vector valued problems, multiclass, multiview learning
etc. I will present a framework for multi-task learning which is based
on learning a common kernel for all tasks. I will also show how this
formulation connects to the trace norm and group Lasso approaches.
Moreover, the proposed optimization problem can be solved using an
alternating minimization algorithm which is simple and efficient. It
can also be "kernelized" by virtue of a multi-task representer theorem,
which holds for a large family of matrix regularization problems and
includes the classical representer theorem as a special case. 
</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="cofee2"></a>04:50 - 05:20</td>
    <td width="90%"><b>Coffee Break</b>
 </td>
 </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="talk3"></a>05:20 - 06:00</td>
    <td width="90%"><b>Learning Vector Fields with Spectral Filtering</b>
[<a href="http://videolectures.net/nipsworkshops09_rosasco_lvfsf/">video</a>]
[<a href="./slides/rosasco.pdf">slides</a>]
 </td>
 </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    <a href="http://web.mit.edu/lrosasco/www/">Lorenzo Rosasco</a>, <i>Massachusetts Institute of Technology and Universita' di Genova</i></td>
  </tr>

  <tr>
    <td width="10%" >&nbsp;</td>
    <td width="90%" >
    We present  a class of regularized kernel methods for vector valued
learning,  which are based on  filtering the spectrum of the kernel
matrix. The considered 	       		     	      methods include
Tikhonov regularization as a special case, as well as interesting
alternatives such as vector valued extensions of L2 boosting. While
preserving the good statistical properties of Tikhonov regularization,
some  of the new algorithms allows for a much faster implementation
since they require 	    	       	      only matrix  vector
multiplications. We discuss the computational complexity of the
different methods,  taking into account the regularization parameter
choice step. The results of our analysis are supported  by numerical experiments.
</td>
  </tr>
</table>
<table width="100%">
  <tr>
    <td width="10%" ><a name="discussion"></a>06:00 - 06:30</td>
    <td width="90%"><b>Discussion session</b>
 </td>
 </tr>
</table>
